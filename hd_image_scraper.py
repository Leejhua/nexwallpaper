#!/usr/bin/env python3
"""
é«˜æ¸…å›¾ç‰‡çˆ¬è™«
ä»labubuwallpaper.xyzè·å–çœŸæ­£çš„é«˜æ¸…å›¾ç‰‡URL
"""

import requests
import json
import re
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os

class HDImageScraper:
    def __init__(self):
        self.base_url = "https://www.labubuwallpaper.xyz"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.all_images = []
        self.all_videos = []
    
    def get_main_page_data(self):
        """è·å–ä¸»é¡µçš„å›¾ç‰‡æ•°æ®"""
        print("ğŸ” è·å–ä¸»é¡µæ•°æ®...")
        
        try:
            response = self.session.get(f"{self.base_url}/?ref=producthunt")
            response.raise_for_status()
            
            # æŸ¥æ‰¾Next.jsæ•°æ®
            soup = BeautifulSoup(response.text, 'html.parser')
            script_tag = soup.find('script', {'id': '__NEXT_DATA__'})
            
            if script_tag:
                data = json.loads(script_tag.string)
                images = data.get('props', {}).get('pageProps', {}).get('images', [])
                print(f"ğŸ“Š ä»ä¸»é¡µè·å–åˆ° {len(images)} ä¸ªåª’ä½“æ–‡ä»¶")
                return images
            else:
                print("âŒ æœªæ‰¾åˆ°Next.jsæ•°æ®")
                return []
                
        except Exception as e:
            print(f"âŒ è·å–ä¸»é¡µæ•°æ®å¤±è´¥: {e}")
            return []
    
    def get_detail_page_hd_url(self, asset_id):
        """è·å–è¯¦æƒ…é¡µé¢çš„é«˜æ¸…å›¾ç‰‡URL"""
        detail_url = f"{self.base_url}/p/{asset_id}"
        
        try:
            response = self.session.get(detail_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾é«˜æ¸…å›¾ç‰‡URL
            # æ–¹æ³•1: æŸ¥æ‰¾og:image metaæ ‡ç­¾
            og_image = soup.find('meta', {'property': 'og:image'})
            if og_image:
                hd_url = og_image.get('content')
                if hd_url and 'res.labubuwallpaper.xyz' in hd_url:
                    # ç§»é™¤å‹ç¼©å‚æ•°ï¼Œè·å–åŸå›¾
                    hd_url = re.sub(r',w_\d+', '', hd_url)
                    hd_url = re.sub(r'/f_auto,q_auto', '', hd_url)
                    return hd_url
            
            # æ–¹æ³•2: æŸ¥æ‰¾imgæ ‡ç­¾ä¸­çš„é«˜åˆ†è¾¨ç‡URL
            img_tags = soup.find_all('img')
            for img in img_tags:
                src = img.get('src', '')
                if 'res.labubuwallpaper.xyz' in src and 'labubu' in src:
                    # ç§»é™¤å‹ç¼©å‚æ•°
                    hd_url = re.sub(r',w_\d+', '', src)
                    hd_url = re.sub(r'/f_auto,q_auto', '', hd_url)
                    return hd_url
            
            # æ–¹æ³•3: æŸ¥æ‰¾Next.jsæ•°æ®ä¸­çš„åŸå§‹URL
            script_tag = soup.find('script', {'id': '__NEXT_DATA__'})
            if script_tag:
                data = json.loads(script_tag.string)
                # æŸ¥æ‰¾å›¾ç‰‡æ•°æ®
                page_props = data.get('props', {}).get('pageProps', {})
                if 'image' in page_props:
                    public_id = page_props['image'].get('public_id', '')
                    format_ext = page_props['image'].get('format', 'jpg')
                    if public_id:
                        return f"https://res.labubuwallpaper.xyz/image/upload/{public_id}.{format_ext}"
            
            return None
            
        except Exception as e:
            print(f"âŒ è·å–è¯¦æƒ…é¡µå¤±è´¥ {asset_id}: {e}")
            return None
    
    def process_media_item(self, item):
        """å¤„ç†å•ä¸ªåª’ä½“é¡¹ç›®"""
        asset_id = item.get('asset_id', '')
        title = item.get('metadata', {}).get('en_title', 'Unknown')
        public_id = item.get('public_id', '')
        format_ext = item.get('format', 'jpg')
        tags = item.get('tags', [])
        resource_type = item.get('resource_type', 'image')
        
        print(f"ğŸ”„ å¤„ç†: {title}")
        
        # æ„å»ºé«˜æ¸…URL
        if public_id:
            if resource_type == 'video':
                hd_url = f"https://res.labubuwallpaper.xyz/video/upload/{public_id}.{format_ext}"
            else:
                hd_url = f"https://res.labubuwallpaper.xyz/image/upload/{public_id}.{format_ext}"
        else:
            # å°è¯•ä»è¯¦æƒ…é¡µè·å–
            hd_url = self.get_detail_page_hd_url(asset_id)
        
        if not hd_url:
            print(f"âš ï¸  æ— æ³•è·å–é«˜æ¸…URL: {title}")
            return None
        
        # åˆ†ç±»æ˜ å°„
        category = self.determine_category(tags, title)
        
        # åˆ†è¾¨ç‡ä¿¡æ¯
        resolution = self.determine_resolution(tags, item)
        
        media_data = {
            'url': hd_url,
            'title': title,
            'category': category,
            'resolution': resolution,
            'source': 'xyz',
            'type': resource_type,
            'asset_id': asset_id,
            'format': format_ext,
            'tags': tags
        }
        
        return media_data
    
    def determine_category(self, tags, title):
        """æ ¹æ®æ ‡ç­¾å’Œæ ‡é¢˜ç¡®å®šåˆ†ç±»"""
        tags_lower = [tag.lower() for tag in tags]
        title_lower = title.lower()
        
        if 'live' in tags_lower or 'live' in title_lower:
            return 'live'
        elif '4k' in tags_lower or '4k' in title_lower:
            return '4k'
        elif 'phone' in tags_lower or 'iphone' in tags_lower or 'mobile' in title_lower:
            return 'mobile'
        elif 'pc' in tags_lower or 'desktop' in tags_lower or 'computer' in title_lower:
            return 'desktop'
        elif any(season in title_lower for season in ['spring', 'summer', 'fall', 'winter', 'christmas', 'halloween']):
            return 'seasonal'
        else:
            return 'fantasy'
    
    def determine_resolution(self, tags, item):
        """ç¡®å®šåˆ†è¾¨ç‡ä¿¡æ¯"""
        tags_lower = [tag.lower() for tag in tags]
        
        if '4k' in tags_lower:
            return '4K'
        elif 'phone' in tags_lower or 'iphone' in tags_lower:
            return 'iPhone'
        elif 'pc' in tags_lower:
            return 'PC'
        elif 'desktop' in tags_lower:
            return 'æ¡Œé¢'
        else:
            # å°è¯•ä»å°ºå¯¸ä¿¡æ¯è·å–
            width = item.get('width', 0)
            height = item.get('height', 0)
            
            if width >= 3840 or height >= 2160:
                return '4K'
            elif width >= 1920 or height >= 1080:
                return 'é«˜æ¸…'
            else:
                return 'æ ‡æ¸…'
    
    def scrape_all_media(self):
        """çˆ¬å–æ‰€æœ‰åª’ä½“æ–‡ä»¶"""
        print("ğŸš€ å¼€å§‹çˆ¬å–é«˜æ¸…åª’ä½“æ–‡ä»¶...")
        
        # è·å–ä¸»é¡µæ•°æ®
        main_page_items = self.get_main_page_data()
        
        if not main_page_items:
            print("âŒ æ— æ³•è·å–ä¸»é¡µæ•°æ®")
            return
        
        print(f"ğŸ“Š å¼€å§‹å¤„ç† {len(main_page_items)} ä¸ªåª’ä½“æ–‡ä»¶...")
        
        for i, item in enumerate(main_page_items, 1):
            print(f"\n[{i}/{len(main_page_items)}]", end=" ")
            
            media_data = self.process_media_item(item)
            
            if media_data:
                if media_data['type'] == 'video':
                    self.all_videos.append(media_data)
                    print(f"âœ… è§†é¢‘: {media_data['title']}")
                else:
                    self.all_images.append(media_data)
                    print(f"âœ… å›¾ç‰‡: {media_data['title']}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“¸ å›¾ç‰‡: {len(self.all_images)} å¼ ")
        print(f"ğŸ¬ è§†é¢‘: {len(self.all_videos)} ä¸ª")
    
    def save_to_js_file(self, filename='hd_gallery_data.js'):
        """ä¿å­˜ä¸ºJavaScriptæ•°æ®æ–‡ä»¶"""
        print(f"ğŸ’¾ ä¿å­˜æ•°æ®åˆ° {filename}...")
        
        js_content = f"""// é«˜æ¸…Labubuå£çº¸æ•°æ® - çœŸæ­£çš„é«˜æ¸…ç‰ˆæœ¬
// çˆ¬å–æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
// å›¾ç‰‡æ•°é‡: {len(self.all_images)}
// è§†é¢‘æ•°é‡: {len(self.all_videos)}

const hdImageData = [
"""
        
        for item in self.all_images:
            js_content += f"""    {{
        url: "{item['url']}",
        title: "{item['title']}",
        category: "{item['category']}",
        resolution: "{item['resolution']}",
        source: "{item['source']}",
        type: "{item['type']}",
        format: "{item['format']}"
    }},
"""
        
        js_content += """];

const hdVideoData = [
"""
        
        for item in self.all_videos:
            js_content += f"""    {{
        url: "{item['url']}",
        title: "{item['title']}",
        category: "{item['category']}",
        resolution: "{item['resolution']}",
        source: "{item['source']}",
        type: "{item['type']}",
        format: "{item['format']}"
    }},
"""
        
        js_content += "];"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(js_content)
        
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {filename}")
    
    def save_analysis_report(self, filename='hd_scraping_report.md'):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        print(f"ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š {filename}...")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_items = len(self.all_images) + len(self.all_videos)
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        categories = {}
        for item in self.all_images + self.all_videos:
            cat = item['category']
            categories[cat] = categories.get(cat, 0) + 1
        
        # æŒ‰åˆ†è¾¨ç‡ç»Ÿè®¡
        resolutions = {}
        for item in self.all_images + self.all_videos:
            res = item['resolution']
            resolutions[res] = resolutions.get(res, 0) + 1
        
        report = f"""# ğŸ” é«˜æ¸…å›¾ç‰‡çˆ¬å–æŠ¥å‘Š

## ğŸ“Š çˆ¬å–ç»Ÿè®¡
- **çˆ¬å–æ—¶é—´**: {time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
- **æ€»è®¡**: {total_items} ä¸ªåª’ä½“æ–‡ä»¶
- **å›¾ç‰‡**: {len(self.all_images)} å¼ 
- **è§†é¢‘**: {len(self.all_videos)} ä¸ª

## ğŸ“‚ åˆ†ç±»åˆ†å¸ƒ
"""
        
        for category, count in sorted(categories.items()):
            percentage = (count / total_items) * 100
            report += f"- **{category}**: {count} ä¸ª ({percentage:.1f}%)\n"
        
        report += f"""
## ğŸ“ åˆ†è¾¨ç‡åˆ†å¸ƒ
"""
        
        for resolution, count in sorted(resolutions.items()):
            percentage = (count / total_items) * 100
            report += f"- **{resolution}**: {count} ä¸ª ({percentage:.1f}%)\n"
        
        report += f"""
## ğŸ”— URLç¤ºä¾‹

### é«˜æ¸…å›¾ç‰‡URLæ ¼å¼
```
https://res.labubuwallpaper.xyz/image/upload/[public_id].[format]
```

### è§†é¢‘URLæ ¼å¼
```
https://res.labubuwallpaper.xyz/video/upload/[public_id].[format]
```

## âœ¨ æ”¹è¿›ç‚¹
1. **çœŸæ­£çš„é«˜æ¸…**: ç§»é™¤äº†æ‰€æœ‰å‹ç¼©å‚æ•°ï¼Œè·å–åŸå§‹é«˜æ¸…å›¾ç‰‡
2. **å®Œæ•´æ•°æ®**: åŒ…å«äº†æ‰€æœ‰å¯ç”¨çš„åª’ä½“æ–‡ä»¶
3. **å‡†ç¡®åˆ†ç±»**: åŸºäºæ ‡ç­¾å’Œæ ‡é¢˜çš„æ™ºèƒ½åˆ†ç±»
4. **è¯¦ç»†ä¿¡æ¯**: åŒ…å«æ ¼å¼ã€åˆ†è¾¨ç‡ç­‰å®Œæ•´å…ƒæ•°æ®

## ğŸ¯ ä½¿ç”¨å»ºè®®
- é«˜æ¸…å›¾ç‰‡é€‚åˆæ¡Œé¢å£çº¸å’Œæ‰“å°
- è§†é¢‘æ–‡ä»¶é€‚åˆåŠ¨æ€å£çº¸
- æ ¹æ®è®¾å¤‡é€‰æ‹©åˆé€‚çš„åˆ†è¾¨ç‡
- æ³¨æ„æ–‡ä»¶å¤§å°ï¼Œé«˜æ¸…å›¾ç‰‡é€šå¸¸è¾ƒå¤§

---
**ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜åˆ° {filename}")

def main():
    print("ğŸ° é«˜æ¸…Labubuå£çº¸çˆ¬è™«")
    print("=" * 50)
    
    scraper = HDImageScraper()
    
    try:
        # çˆ¬å–æ‰€æœ‰åª’ä½“
        scraper.scrape_all_media()
        
        # ä¿å­˜æ•°æ®
        scraper.save_to_js_file()
        scraper.save_analysis_report()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ é«˜æ¸…å›¾ç‰‡çˆ¬å–å®Œæˆ!")
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("   â€¢ hd_gallery_data.js - é«˜æ¸…æ•°æ®æ–‡ä»¶")
        print("   â€¢ hd_scraping_report.md - çˆ¬å–æŠ¥å‘Š")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        print(f"\nâŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")

if __name__ == "__main__":
    main()
