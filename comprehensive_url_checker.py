#!/usr/bin/env python3
"""
å…¨é¢URLæ£€æµ‹å’Œæ›´æ–°è„šæœ¬
æ£€æµ‹æ‰€æœ‰åˆ†ç±»çš„å›¾ç‰‡å’Œè§†é¢‘URLï¼Œç”Ÿæˆæ›´æ–°çš„JSONæ•°æ®æ–‡ä»¶
"""

import requests
import re
import time
import json
from datetime import datetime
from urllib.parse import quote

class ComprehensiveURLChecker:
    def __init__(self):
        self.all_media = []
        self.results = {
            'working_urls': [],
            'broken_urls': [],
            'statistics': {},
            'last_updated': datetime.now().isoformat()
        }
        self.load_data()
    
    def load_data(self):
        """ä»æ•°æ®æ–‡ä»¶åŠ è½½åª’ä½“æ•°æ®"""
        print("ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶...")
        
        with open('complete_gallery_data.js', 'r', encoding='utf-8') as f:
            content = f.read()
        
        # è§£ææ•°æ®
        lines = content.split('\n')
        current_item = {}
        in_image_data = False
        in_video_data = False
        
        for line in lines:
            line = line.strip()
            
            if 'completeImageData' in line:
                in_image_data = True
                continue
            elif 'completeVideoData' in line:
                in_image_data = False
                in_video_data = True
                continue
            
            if in_image_data or in_video_data:
                if 'url:' in line and '"' in line:
                    url_match = re.search(r'url:\s*"([^"]*)"', line)
                    if url_match:
                        current_item['url'] = url_match.group(1)
                elif 'title:' in line and '"' in line:
                    title_match = re.search(r'title:\s*"([^"]*)"', line)
                    if title_match:
                        current_item['title'] = title_match.group(1)
                elif 'category:' in line and '"' in line:
                    category_match = re.search(r'category:\s*"([^"]*)"', line)
                    if category_match:
                        current_item['category'] = category_match.group(1)
                elif 'source:' in line and '"' in line:
                    source_match = re.search(r'source:\s*"([^"]*)"', line)
                    if source_match:
                        current_item['source'] = source_match.group(1)
                elif 'resolution:' in line and '"' in line:
                    resolution_match = re.search(r'resolution:\s*"([^"]*)"', line)
                    if resolution_match:
                        current_item['resolution'] = resolution_match.group(1)
                elif line == '},' or line == '}':
                    if 'url' in current_item and 'title' in current_item:
                        current_item['type'] = 'video' if (current_item['url'].endswith('.mp4') or current_item['url'].endswith('.mov')) else 'image'
                        self.all_media.append(current_item.copy())
                    current_item = {}
        
        print(f"ğŸ“Š åŠ è½½å®Œæˆ: {len(self.all_media)} ä¸ªåª’ä½“æ–‡ä»¶")
    
    def check_url(self, url, timeout=10):
        """æ£€æŸ¥å•ä¸ªURLçš„å¯è®¿é—®æ€§"""
        try:
            response = requests.head(url, timeout=timeout, allow_redirects=True)
            return response.status_code == 200, response.status_code
        except requests.exceptions.RequestException as e:
            return False, str(e)
    
    def check_all_urls(self):
        """æ£€æŸ¥æ‰€æœ‰URL"""
        print("ğŸ” å¼€å§‹å…¨é¢URLæ£€æµ‹...")
        
        categories = {}
        sources = {}
        
        for i, item in enumerate(self.all_media, 1):
            url = item['url']
            title = item.get('title', 'Unknown')
            category = item.get('category', 'unknown')
            source = item.get('source', 'unknown')
            media_type = item.get('type', 'unknown')
            
            print(f"\n[{i}/{len(self.all_media)}] æ£€æµ‹: {title[:40]}...")
            print(f"åˆ†ç±»: {category} | æ¥æº: {source} | ç±»å‹: {media_type}")
            print(f"URL: {url}")
            
            is_working, status = self.check_url(url)
            
            # ç»Ÿè®¡ä¿¡æ¯
            if category not in categories:
                categories[category] = {'total': 0, 'working': 0, 'broken': 0}
            if source not in sources:
                sources[source] = {'total': 0, 'working': 0, 'broken': 0}
            
            categories[category]['total'] += 1
            sources[source]['total'] += 1
            
            if is_working:
                print(f"âœ… æ­£å¸¸ (çŠ¶æ€: {status})")
                self.results['working_urls'].append({
                    'url': url,
                    'title': title,
                    'category': category,
                    'source': source,
                    'type': media_type,
                    'status': status
                })
                categories[category]['working'] += 1
                sources[source]['working'] += 1
            else:
                print(f"âŒ å¤±æ•ˆ (çŠ¶æ€: {status})")
                self.results['broken_urls'].append({
                    'url': url,
                    'title': title,
                    'category': category,
                    'source': source,
                    'type': media_type,
                    'error': status
                })
                categories[category]['broken'] += 1
                sources[source]['broken'] += 1
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        self.results['statistics'] = {
            'by_category': categories,
            'by_source': sources,
            'total': len(self.all_media),
            'working': len(self.results['working_urls']),
            'broken': len(self.results['broken_urls'])
        }
    
    def generate_report(self):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“‹ å…¨é¢URLæ£€æµ‹æŠ¥å‘Š")
        print("="*80)
        
        stats = self.results['statistics']
        total = stats['total']
        working = stats['working']
        broken = stats['broken']
        
        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"   æ€»è®¡: {total} ä¸ªåª’ä½“æ–‡ä»¶")
        print(f"   æ­£å¸¸: {working} ä¸ª ({working/total*100:.1f}%)")
        print(f"   å¤±æ•ˆ: {broken} ä¸ª ({broken/total*100:.1f}%)")
        
        print(f"\nğŸ“‚ æŒ‰åˆ†ç±»ç»Ÿè®¡:")
        for category, data in stats['by_category'].items():
            success_rate = data['working']/data['total']*100 if data['total'] > 0 else 0
            print(f"   {category}: {data['working']}/{data['total']} ({success_rate:.1f}%)")
        
        print(f"\nğŸŒ æŒ‰æ¥æºç»Ÿè®¡:")
        for source, data in stats['by_source'].items():
            success_rate = data['working']/data['total']*100 if data['total'] > 0 else 0
            print(f"   {source}: {data['working']}/{data['total']} ({success_rate:.1f}%)")
        
        if self.results['broken_urls']:
            print(f"\nâŒ å¤±æ•ˆURLè¯¦æƒ…:")
            for i, item in enumerate(self.results['broken_urls'], 1):
                print(f"{i}. {item['title']}")
                print(f"   åˆ†ç±»: {item['category']} | æ¥æº: {item['source']} | ç±»å‹: {item['type']}")
                print(f"   URL: {item['url']}")
                print(f"   é”™è¯¯: {item['error']}")
                print()
    
    def generate_updated_data_file(self):
        """ç”Ÿæˆæ›´æ–°åçš„æ•°æ®æ–‡ä»¶"""
        print("ğŸ“ ç”Ÿæˆæ›´æ–°åçš„æ•°æ®æ–‡ä»¶...")
        
        # åˆ†ç¦»å›¾ç‰‡å’Œè§†é¢‘
        working_images = [item for item in self.results['working_urls'] if item['type'] == 'image']
        working_videos = [item for item in self.results['working_urls'] if item['type'] == 'video']
        
        # ç”Ÿæˆæ–°çš„JavaScriptæ•°æ®æ–‡ä»¶
        js_content = f"""// æ›´æ–°çš„Labubuå£çº¸æ•°æ® - å·²ç§»é™¤å¤±æ•ˆURL
// æœ€åæ›´æ–°æ—¶é—´: {self.results['last_updated']}
// ç»Ÿè®¡: {len(working_images)} å¼ å›¾ç‰‡, {len(working_videos)} ä¸ªè§†é¢‘

const completeImageData = [
"""
        
        for item in working_images:
            js_content += f"""    {{
        url: "{item['url']}",
        title: "{item['title']}",
        category: "{item['category']}",
        resolution: "{item.get('resolution', 'é«˜æ¸…')}",
        source: "{item['source']}",
        type: "åŸå›¾"
    }},
"""
        
        js_content += """];

const completeVideoData = [
"""
        
        for item in working_videos:
            js_content += f"""    {{
        url: "{item['url']}",
        title: "{item['title']}",
        category: "{item['category']}",
        source: "{item['source']}"
    }},
"""
        
        js_content += "];"
        
        # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
        with open('complete_gallery_data_updated.js', 'w', encoding='utf-8') as f:
            f.write(js_content)
        
        print("âœ… å·²ç”Ÿæˆ complete_gallery_data_updated.js")
    
    def save_json_report(self):
        """ä¿å­˜JSONæ ¼å¼çš„æ£€æµ‹æŠ¥å‘Š"""
        with open('url_check_report.json', 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print("âœ… å·²ä¿å­˜ url_check_report.json")

def main():
    print("ğŸš€ å¯åŠ¨å…¨é¢URLæ£€æµ‹å’Œæ›´æ–°ç³»ç»Ÿ")
    print("="*50)
    
    checker = ComprehensiveURLChecker()
    
    # æ‰§è¡Œæ£€æµ‹
    checker.check_all_urls()
    
    # ç”ŸæˆæŠ¥å‘Š
    checker.generate_report()
    
    # ç”Ÿæˆæ›´æ–°æ–‡ä»¶
    checker.generate_updated_data_file()
    checker.save_json_report()
    
    print("\n" + "="*50)
    print("ğŸ‰ æ£€æµ‹å’Œæ›´æ–°å®Œæˆ!")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("   â€¢ complete_gallery_data_updated.js - æ›´æ–°åçš„æ•°æ®æ–‡ä»¶")
    print("   â€¢ url_check_report.json - è¯¦ç»†æ£€æµ‹æŠ¥å‘Š")

if __name__ == "__main__":
    main()
